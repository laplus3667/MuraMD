
📝【DP-SGD 數學公式解析】
━━━━━━━━━━━━━━━━━━━━━━━━

🧮 **數學公式的清晰性**

當我們解析 DP-SGD 的數學公式，最令人印象深刻的是如何明確地表示批次梯度 Batch Gradient。其中，`N` 代表總的訓練樣本 Training Sample，而 `B` 則代表批大小 Batch Size。進一步，取樣率 Sampling Rate 定義為 `q = B/N`。

🔍 **噪化梯度的定義**

$$
g:=\frac{1}{B} \sum_{i \in B} \operatorname{clip}_C\left(\nabla_\theta \ell_i(\theta)\right)+\mathcal{N}\left(0, \frac{C^2 \sigma^2}{B^2}\right)
$$

這個公式簡單明瞭地展示了噪化梯度的計算方式！

📈 為了深入理解，我考慮搭配羅吉斯迴歸 Logistic Regression 的梯度函數 Gradient Function 進行分析，使其更為詳細和透徹。

🔐 另外，為了執行隱私分析 Privacy Analysis，我也打算深入研究 Renyi 差分隱私 Rényi Differential Privacy (RDP) 的框架，這是我目前想集中精力的重點。

